{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "QM_2NAkAcFnE",
        "d7givTSbBzb9",
        "D7R2pJXrCX_c",
        "TFp-RcldCEpa"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulodowd/woiec2023/blob/main/Worksheet_4_Convolutional_Neural_Networks_Sound_Applications.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **A Brief Introduction to Speech Recognition and Sound Classification**\n",
        "\n",
        "---\n",
        "\n",
        "Last worksheet, we covered the basic priniciples of building a machine learning model. In this worksheet, we will explore how we can apply these same principles to different problem areas, to gain an understanding of how we can build powerful models for a range of application areas. Yesterday, we looked at a computer vision based problem, today we will aim to look at a task involving sound.\n",
        "\n",
        "**By the end of this worksheet you will:**\n",
        "1. Understand the importance of appropriate preprocessing steps, and how to research appropriate steps\n",
        "2. Have an understanding of how to approach different problems with a data-driven perspective\n",
        "3. Understand how to go about building a convolutional neural network that classifies sound.\n",
        "\n",
        "### How we hear sound\n",
        "\n",
        "Our ears detect sound by collecting sound waves through the outer ear, which vibrate the eardrum in the middle ear.\n",
        "\n",
        "![Pressure waves and the ear](https://res.cloudinary.com/dk-find-out/image/upload/q_70,c_pad,w_1200,h_630,f_auto/DK_aw_Ear_pkaizy.jpg) These vibrations are then transferred to the cochlea in the inner ear, where hair cells convert them into neural signals. These signals are sent to the brain via the auditory nerve, allowing us to perceive and interpret sound.\n",
        "\n",
        "### How computers hear sound\n",
        "\n",
        "Sound is stored digitally by sampling the analog sound wave at regular intervals and quantising the amplitude of each sample. You might have seen representations of sound waves before - they look something like the image below.\n",
        "\n",
        "![Sound Wave Representation](data:image/jpeg;base64,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)\n",
        "\n",
        "We can convert this waveform into a special type of image called a spectrogram, which looks something akin to this:\n",
        "\n",
        "![Spectrogram Example](https://miro.medium.com/v2/resize:fit:2000/1*V2mgZ7y0ngd3q4DZ01xkEQ.png)\n",
        "\n",
        "#### **What actually are spectrograms?**\n",
        "A spectrogram is a visual representation of the frequency content of a signal over time. It provides a way to analyze and display the varying intensity or amplitude of different frequencies present in a sound signal as it evolves over time. In a spectrogram, the vertical axis represents frequency, the horizontal axis represents time, and the color or grayscale intensity represents the magnitude or energy of each frequency component at a specific time. Brighter or darker areas in the spectrogram indicate higher or lower energy levels respectively, allowing for easy identification of frequency patterns, harmonics, and time-varying spectral characteristics in the sound signal.\n"
      ],
      "metadata": {
        "id": "RRNmShRrBro7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Before you get started\n",
        "\n",
        "The following sections contain information about downloading, exploring, and then preprocessing our sound files into spectrograms. I would recommend reading through the explanatory text and code block outputs, to gain an understanding of the data before moving on to the Model Selection Stage and ensuing tasks.\n"
      ],
      "metadata": {
        "id": "HzmBRh6PDNHk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Required Packages and Download the Data\n",
        "\n",
        "Make sure you run the following code blocks (but don't edit them), to install the right packages, download the dataset we will be using, and to define some useful helper functions."
      ],
      "metadata": {
        "id": "QM_2NAkAcFnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow-io[tensorflow]"
      ],
      "metadata": {
        "id": "zZFIx953Vxkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import required modules\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_io as tfio\n",
        "\n",
        "from pathlib import Path\n",
        "from random import randint\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "F7W3bOFODUop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 1: download the data\n",
        "DATASET_PATH = 'data/mini_speech_commands'\n",
        "\n",
        "data_dir = Path(DATASET_PATH)\n",
        "if not data_dir.exists():\n",
        "  tf.keras.utils.get_file(\n",
        "      'mini_speech_commands.zip',\n",
        "      origin=\"http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\",\n",
        "      extract=True,\n",
        "      cache_dir='.', cache_subdir='data')"
      ],
      "metadata": {
        "id": "ZkK4MrbOBi40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_sound_file(sound_filepath):\n",
        "    \"\"\" load the raw sound files \"\"\"\n",
        "\n",
        "    # load file\n",
        "    file_contents = tf.io.read_file(sound_filepath)\n",
        "    wav, sample_rate = tf.audio.decode_wav(file_contents, desired_channels=1)\n",
        "\n",
        "    # correct shape\n",
        "    wav = tf.squeeze(wav, axis=-1)\n",
        "    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
        "    # G6000hz = sound_filepath amplitude\n",
        "    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)\n",
        "    return wav\n"
      ],
      "metadata": {
        "id": "M6WjtpE2GDGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stage 1: Data Preparation\n",
        "\n",
        "The dataset's audio clips are stored in eight folders corresponding to each speech command: no, yes, down, go, left, up, right, and stop. You can view these in the sidebar by clicking the file icon.\n",
        "\n",
        "The [Dataset Paper](https://arxiv.org/pdf/1804.03209.pdf) is available on arxiv. This paper provides further information about the dataset."
      ],
      "metadata": {
        "id": "d7givTSbBzb9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Exploring the data"
      ],
      "metadata": {
        "id": "Y-OrBmspQMMS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data should already by downloaded if you have run the helper functions. If the running the cell below causes an error, try running the helper functions first.\n",
        "\n",
        "**Tasks:**\n",
        "1. Choose which two classes to examine. Make changes to the code block below based on the classes you have chosen to investigate, and plot the raw waveform data.\n",
        "2. Refer to the dataset paper to determine key characteristics of the data, that will aid us in ensuring we are preprocessing it accurately."
      ],
      "metadata": {
        "id": "rk6EQWWlGEge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialise key filepaths\n",
        "data_folder = os.path.join(os.getcwd(), \"data\", \"mini_speech_commands\")\n",
        "class_folders = [os.path.join(data_folder, i) for i in os.listdir(data_folder)]\n",
        "\n",
        "# select what classes we will train on, you can choose from:\n",
        "# \"go\",\n",
        "word_1 = \"go\"\n",
        "word_2 = \"stop\"\n",
        "labels = [word_1, word_2]\n",
        "dataset_class_folders = []\n",
        "\n",
        "for label in labels:\n",
        "  for class_folder in class_folders:\n",
        "    if label in class_folder:\n",
        "      dataset_class_folders.append(class_folder)\n",
        "\n",
        "\n",
        "# plot examples of data\n",
        "fig, axes = plt.subplots(2, 4, figsize=(16, 9))\n",
        "\n",
        "for i in range(0, len(dataset_class_folders)):\n",
        "  for j in range (0,4):\n",
        "\n",
        "    sound_filename = os.path.join(dataset_class_folders[i], os.listdir(dataset_class_folders[i])[j]) # get the first file in the directory\n",
        "    wav = load_sound_file(sound_filename)\n",
        "\n",
        "    title = os.path.basename(dataset_class_folders[i])\n",
        "    wav_np = wav.numpy()\n",
        "    ax = axes[i, j]\n",
        "    ax.plot(wav_np)\n",
        "\n",
        "    if word_1 in sound_filename:\n",
        "      ax.set_title(word_1)\n",
        "    elif word_2 in sound_filename:\n",
        "      ax.set_title(word_2)\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3OJ1XYvZmm7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What deductions can you make about the data?\n",
        "\n",
        "\n",
        "\n",
        "1. The sound clips are not all the same length. Most ML/DL models take inputs of a fixed size, so we will need to address this at the preprocessing stage.\n",
        "\n",
        "You can uncomment the code block below to investigate the length of the sound sequences for your chosen classes by changing the variable `chosen_word`. The `load_sound_file` function returns the wav files as a Tensor, so you may find the [Tensor documentation](https://www.tensorflow.org/api_docs/python/tf/Tensor) helpful here."
      ],
      "metadata": {
        "id": "SCGypUkRLBec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lengths = []\n",
        "# chosen_word = word_2\n",
        "# for file in os.listdir(os.path.join('data', 'mini_speech_commands', chosen_word)):\n",
        "\n",
        "#     tensor_wave = load_sound_file(os.path.join('data', 'mini_speech_commands', chosen_word, file))\n",
        "#     lengths.append(tensor_wave.__len__())\n",
        "\n",
        "# myset = set(lengths)\n",
        "# print(myset)"
      ],
      "metadata": {
        "id": "-aqmPtOyoDp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring appropriate data representations\n",
        "\n",
        "We can use the raw 1D waveform, or spectrogram as input to our ML models, or look to alternative representations, such as spectrograms. Research papers provide a useful source of inspiration when approaching our own DL problems. Many papers describe sound classification, and include key details about the data preprocessing, and model architecture.\n",
        "\n"
      ],
      "metadata": {
        "id": "pcy9ObJmTT8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess the Data into Spectrograms\n",
        "\n",
        "These helper functions convert the raw waveform for the chosen classes into a spectrogram. This code block will take a few minutes to run."
      ],
      "metadata": {
        "id": "EfFNgMMEtq3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(file_path, label):\n",
        "  spectrograms, labels = [], []\n",
        "\n",
        "  for i in os.listdir(file_path):\n",
        "\n",
        "    i = os.path.join(file_path, i)\n",
        "    wav = load_sound_file(i)\n",
        "\n",
        "    # ensure all sequences are the same length\n",
        "    wav = wav[:16000]\n",
        "    zero_padding = tf.zeros([16000] - tf.shape(wav), dtype=tf.float32)\n",
        "    wav = tf.concat([zero_padding, wav],0)\n",
        "\n",
        "    # preprocessing for waveforms here\n",
        "    spectrogram = tf.signal.stft(wav, frame_length=255, frame_step=320)\n",
        "\n",
        "    # leave these here\n",
        "    spectrogram = tf.abs(spectrogram)\n",
        "    spectrogram = tf.expand_dims(spectrogram, axis=2)\n",
        "\n",
        "    spectrograms.append(spectrogram)\n",
        "    labels.append(label)\n",
        "\n",
        "  return spectrograms, labels\n",
        "\n",
        "# Defining the positive and negative paths\n",
        "root_folder_go_data = os.path.join('data', 'mini_speech_commands', word_1)\n",
        "root_folder_stop_data = os.path.join('data', 'mini_speech_commands', word_2)\n",
        "\n",
        "go_filepaths = [os.path.join(root_folder_go_data, i) for i in os.listdir(root_folder_go_data)]\n",
        "stop_filepaths = [os.path.join(root_folder_stop_data, i) for i in os.listdir(root_folder_stop_data)]\n",
        "\n",
        "# Creating the Database\n",
        "go_spectrograms, go_labels = preprocess(root_folder_go_data, 0)\n",
        "stop_spectrograms, stop_labels = preprocess(root_folder_stop_data, 1)\n",
        "\n",
        "\n",
        "# concatenate database\n",
        "spectrogram_data = np.array(go_spectrograms + stop_spectrograms)\n",
        "\n",
        "label_data = np.array(go_labels + stop_labels)\n",
        "\n",
        "# label_data = pd.get_dummies(label_data)\n",
        "# print(label_data.head())\n",
        "h, w, c = spectrogram_data[0].shape"
      ],
      "metadata": {
        "id": "FqR8W056pSHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_spectrogram(spectrogram, ax):\n",
        "  if len(spectrogram.shape) > 2:\n",
        "    assert len(spectrogram.shape) == 3\n",
        "    spectrogram = np.squeeze(spectrogram, axis=-1)\n",
        "  # Convert the frequencies to log scale and transpose, so that the time is\n",
        "  # represented on the x-axis (columns).\n",
        "  # Add an epsilon to avoid taking a log of zero.\n",
        "  log_spec = np.log(spectrogram.T + np.finfo(float).eps)\n",
        "  height = log_spec.shape[0]\n",
        "  width = log_spec.shape[1]\n",
        "  X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n",
        "  Y = range(height)\n",
        "  ax.pcolormesh(X, Y, log_spec)\n",
        "\n",
        "n_plots=4\n",
        "\n",
        "class_for_title = 0\n",
        "for spectrogram_class in [go_spectrograms, stop_spectrograms]:\n",
        "\n",
        "  fig, axes = plt.subplots(n_plots)\n",
        "  for i in range(0,n_plots):\n",
        "    plot_spectrogram(spectrogram_class[i].numpy(), axes[i])\n",
        "    axes[0].set_title('Class ' + str(class_for_title))\n",
        "  class_for_title +=1\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "kMfjk4a2uvjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 2: Model Selection"
      ],
      "metadata": {
        "id": "vLHy0e_UCA8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Build Model\n",
        "\n",
        "Here we will focus on building a simple baseline model that works end-to-end. We'll use similar code to the model that we built yesterday - review the model, and familiarise yourself with its architecture by running the code block below and examining the output."
      ],
      "metadata": {
        "id": "D7R2pJXrCX_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(nrows, ncols, nchannels):\n",
        "\n",
        "  # define our input layer\n",
        "  inputs = keras.Input(shape=(nrows, ncols, nchannels))\n",
        "\n",
        "  # define the core model: a 2d-convolutional layer, followed by a max pooling layer\n",
        "  conv2d = keras.layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n",
        "  pooling = keras.layers.MaxPooling2D((2, 2))(conv2d)\n",
        "\n",
        "  # flatten the output from the convolutional block\n",
        "  flatten = keras.layers.Flatten()(pooling)\n",
        "\n",
        "  # define output layer\n",
        "  outputs = keras.layers.Dense(1, activation=\"sigmoid\")(flatten)\n",
        "\n",
        "  # build the model\n",
        "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  return model\n",
        "\n",
        "build_model(h,w,c)"
      ],
      "metadata": {
        "id": "IwNDEJ9KDIPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model is very simple, and worked relatively well for the very simple MNIST data. We'll use it as a baseline, and can later refine it if necessary."
      ],
      "metadata": {
        "id": "rtQzfkD1Gh76"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Train\n",
        "\n",
        "Once again we will use the same training code as from our worksheet yesterday."
      ],
      "metadata": {
        "id": "TFp-RcldCEpa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training loop"
      ],
      "metadata": {
        "id": "vgcD80XXt37g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(spectrogram_data, label_data, test_size=0.2, shuffle=True, random_state=42)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "WZfaT8eUsxsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training procedure: we will use cross validation to get a better idea of how our model is performing\n",
        "n_folds = 5\n",
        "kf = KFold(n_folds, shuffle=True, random_state=42)\n",
        "\n",
        "# define optimizer for model compilation\n",
        "lr = 0.01\n",
        "opt = keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "# store model accuracy\n",
        "accuracies = []\n",
        "\n",
        "\n",
        "for i, (train_index, val_index) in enumerate(kf.split(X_train)): # for each fold\n",
        "  print(f\"Fold {i}\")\n",
        "\n",
        "  # retrieve fold data\n",
        "  X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
        "  print(X_train_fold.shape, X_val_fold.shape)\n",
        "  y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
        "\n",
        "  # build model\n",
        "  digit_predictor = build_model(X_train.shape[1], X_train.shape[2], 1)\n",
        "  digit_predictor.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  # define callbacks\n",
        "  callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=0)\n",
        "\n",
        "  # train model\n",
        "  history = digit_predictor.fit(X_train_fold, y_train_fold, epochs=10, batch_size=32, validation_data=(X_val_fold, y_val_fold), callbacks=[callback], verbose=1)\n",
        "\n",
        "  # evaluate performance on the validation set\n",
        "  loss, acc = digit_predictor.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
        "\n",
        "  accuracies.append(acc)\n",
        "\n",
        "  print(f\"Accuracy: {acc}\" )\n",
        "\n"
      ],
      "metadata": {
        "id": "FuCh3PAEIaPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracies"
      ],
      "metadata": {
        "id": "96KVFUIdsAn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Refine the Model\n",
        "\n",
        "Now that we have our data in the correct format, and a simple model working end-to-end, let's see if we can improve the performance. There are a number of ways we can go about this:\n",
        "\n",
        "1. Refining the model architecture\n",
        "2. Tuning the model hyperparameters\n",
        "3. Varying our data preprocessing methods to get different representations of the data\n",
        "\n",
        "\n",
        "\n",
        "**Task:** Improve performance of the network by refining the model architecture in the code block below, and then rerunning the training loop (also below). Adding layers to the model increases its complexity, allowing it to gain a more nuanced understanding of the data. We can tell the model is struggling to learn if the training loss doesn't reduce - the smaller this value is, the better.\n",
        "1. We only have 1 Dense layer at the end of our model, but it's generally considered good practice to have at least two. Add more complexity to the model by adding an additional Dense layer to the model (see code block below for location).\n",
        "2. Train the model again. Has performance improved? Which model performs better?\n",
        "3. Convolutional neural networks have been shown to be more effective on many types of data when two convolutional layers are applied consecutively. Add a second convolutional layer below the first (see code block below for location) Feel free to experiment with the number of filters in the layer.\n",
        "4. Record your changes and the overall accuracy - sometimes simpler models can perform surprisingly well. Once you are happy with the overall accuracy you can move onto testing the model's performance on the test set.\n",
        "\n",
        "Resources:\n",
        "1. The [Keras Functional API](https://keras.io/guides/functional_api/). This explains in more detail how to define the layers.\n",
        "2. [Convolutional Layer documentation](https://keras.io/api/layers/convolution_layers/convolution2d/)\n",
        "3. [Dense Layer documentation](https://keras.io/api/layers/core_layers/dense/)\n",
        "4. Research papers often provide a useful starting point when approaching ML/DL problems - we can look for similar taks/problems, and see what kind of models have performed well before trying some of those architectures. [This](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9088863&casa_token=PDCtcacEHaEAAAAA:Dx7DoClLWRLZ3zhmJXcIuPSKAtJn9GoiI5vB-62WN9UVM6BAJP_0HmwKLvW1DYcYa-GWBnUr9ts) paper provides a couple of ideas - you could try implementing one of the convolutional blocks shown in Figure 3."
      ],
      "metadata": {
        "id": "x0oyTaNXC8D4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_new_model(nrows, ncols, nchannels):\n",
        "\n",
        "  # define our input layer\n",
        "  inputs = keras.Input(shape=(nrows, ncols, nchannels))\n",
        "\n",
        "  # define the core model: a 2d-convolutional layer, followed by a max pooling layer\n",
        "  conv2d = keras.layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n",
        "\n",
        "  #### ADD CONVOLUTIONAL LAYER BELOW THIS LINE: make sure you complete task 1 before filling this out!\n",
        "  ### review the keras documentation for the Conv2D layer, and implement that layer here,\n",
        "  # make sure you link it to the previous conv2d layer by correctly setting the output\n",
        "  # on BOTH the input to the new layer, and the output of the pooling layer, template below\n",
        "\n",
        "  # conv2d = keras.layers.Conv2D(??, ??, activation=\"relu\")(previous_layer)\n",
        "\n",
        "  #### ADD CONV LAYER ABOVE THIS LINE\n",
        "\n",
        "  pooling = keras.layers.MaxPooling2D((2, 2))(conv2d) # don't forget to change this when building more convolutional layers\n",
        "\n",
        "  # flatten the output from the convolutional block\n",
        "  flatten = keras.layers.Flatten()(pooling)\n",
        "\n",
        "  # and define the output layer\n",
        "  ## fully-connected/dense layers\n",
        "\n",
        "  #### ADD DENSE LAYER BELOW THIS LINE\n",
        "  # general format for a new keras layer:\n",
        "  # new_layer = keras.layers.LAYERNAME(.....)(previous_layer)\n",
        "  # where the previous layer is the layer above this one in the model architecture (i.e. flatten)\n",
        "  # dense1 = keras.layers.Dense(??, activation=??)(previous_layer) # don't forget to set previous_layer to the output of your new layer\n",
        "  #### ADD DENSE LAYER ABOVE THIS LINE\n",
        "\n",
        "  outputs = keras.layers.Dense(1, activation=\"sigmoid\")(flatten)\n",
        "\n",
        "  # build the model\n",
        "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  return model\n",
        "\n",
        "build_new_model(h,w,c)"
      ],
      "metadata": {
        "id": "4Ri6AMZjjukq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracies = []\n",
        "\n",
        "for i, (train_index, val_index) in enumerate(kf.split(X_train)): # for each fold\n",
        "  print(f\"Fold {i}\")\n",
        "\n",
        "  # retrieve fold data\n",
        "  X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
        "  print(X_train_fold.shape, X_val_fold.shape)\n",
        "  y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
        "\n",
        "  # build model\n",
        "  digit_predictor = build_new_model(X_train.shape[1], X_train.shape[2], 1)\n",
        "  digit_predictor.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  # define callbacks\n",
        "  callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=0)\n",
        "\n",
        "  # train model\n",
        "  history = digit_predictor.fit(X_train_fold, y_train_fold, epochs=10, batch_size=32, validation_data=(X_val_fold, y_val_fold), callbacks=[callback], verbose=1)\n",
        "\n",
        "  # evaluate performance on the validation set\n",
        "  loss, acc = digit_predictor.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
        "\n",
        "  accuracies.append(acc)\n",
        "\n",
        "  print(f\"Accuracy: {acc}\" )"
      ],
      "metadata": {
        "id": "qy6_BDUIqVz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracies"
      ],
      "metadata": {
        "id": "T9zQBBz5sENn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 3: Evaluate Model Performance on Test Set\n",
        "\n",
        "Once you are happy with your model's performance on the validation set, we can retrain on the full dataset, and make predictions on the test set.\n",
        "\n",
        "**Task:**\n",
        "1. Add the number of epochs you would like to train the model for - choose this based on how many epochs your model typically trained for during the cross validation."
      ],
      "metadata": {
        "id": "YS4jNgqeClTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define key parameters\n",
        "num_epochs = ??\n",
        "batchsize = 32\n",
        "\n",
        "# initialise model\n",
        "digit_predictor = build_model(X_train.shape[1], X_train.shape[2], 1)\n",
        "digit_predictor.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# train on full dataset\n",
        "hist = digit_predictor.fit(X_train, y_train, epochs=num_epochs, batch_size=batchsize)\n",
        "\n",
        "# evaluate performance of model on test set\n",
        "loss, acc = digit_predictor.evaluate(X_test, y_test, verbose=0)\n",
        "print(loss, acc)"
      ],
      "metadata": {
        "id": "vPcgxTqmUD2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How does the model perform on unseen (test set) data? Is this better or worse than expected?"
      ],
      "metadata": {
        "id": "I8_JufFal4H6"
      }
    }
  ]
}